{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Q3vLz6QkIfQ6","MqxhwpA9IkI2","dAqtK5xiT_wb"],"authorship_tag":"ABX9TyPCe8XBS2+EJ8z6DSJ2INNq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Two demos code blocks for computation of input,hidden,... sizes "],"metadata":{"id":"qe6SGBo1Dvqe"}},{"cell_type":"markdown","source":["##Use RNN Cell\n","\n","\n","This block serves as a little demo for computation & verification of input output hidden size seq_len etc. **in senerio of using RNN Cell**"],"metadata":{"id":"Q3vLz6QkIfQ6"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Is9T4rxCadI","executionInfo":{"status":"ok","timestamp":1676108207973,"user_tz":-60,"elapsed":4738,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"9d356caf-9ff4-4677-fc7b-b7584175a221"},"outputs":[{"output_type":"stream","name":"stdout","text":["==================== 0 ====================\n","Input size:  torch.Size([1, 4])\n","outputs size:  torch.Size([1, 2])\n","tensor([[-0.4599, -0.8891]])\n","==================== 1 ====================\n","Input size:  torch.Size([1, 4])\n","outputs size:  torch.Size([1, 2])\n","tensor([[ 0.1246, -0.9255]])\n","==================== 2 ====================\n","Input size:  torch.Size([1, 4])\n","outputs size:  torch.Size([1, 2])\n","tensor([[-0.1339,  0.1697]])\n"]}],"source":["import torch\n","\n","# PARAMERTER SETTINGS\n","\n","batch_size = 1\n","seq_len = 3\n","input_size = 4\n","hidden_size = 2\n","\n","# Here using RNNCell\n","cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n","\n","# Create ａ　dataset :(seq, batch, input_size)\n","dataset = torch.randn(seq_len, batch_size, input_size)\n","\n","# Create h_0 for start\n","hidden = torch.zeros(batch_size, hidden_size) \n","\n","# iterate over seq_len = 3\n","for idx, input in enumerate(dataset,0):   \n","  with torch. no_grad():\n","    print('=' * 20, idx, '=' * 20)\n","    print('Input size: ', input.shape)\n","    hidden = cell(input, hidden)\n","    print('outputs size: ', hidden.shape)\n","    print(hidden)"]},{"cell_type":"markdown","source":["##Use RNN \n","\n","This block serves as a little demo for computation & verification of input output hidden size seq_len etc. in senerio of using RNN Module"],"metadata":{"id":"MqxhwpA9IkI2"}},{"cell_type":"code","source":["import torch\n","batch_size = 1\n","seq_len = 3\n","input_size = 4\n","hidden_size = 2\n","num_layers = 1\n","cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size,\n","num_layers=num_layers)\n","# (seqLen, batchSize, inputSize)\n","inputs = torch.randn(seq_len, batch_size, input_size)\n","hidden = torch.zeros(num_layers, batch_size, hidden_size)\n","with torch. no_grad():\n","  out, hidden = cell(inputs, hidden)\n","  print('Output size:', out.shape)\n","  print('Output:', out)\n","  print('Hidden size: ', hidden.shape)\n","  print('Hidden: ', hidden)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1F7uocNlIeTm","executionInfo":{"status":"ok","timestamp":1676108207973,"user_tz":-60,"elapsed":5,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"f8c70763-ba2e-481d-b010-85a371b77859"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output size: torch.Size([3, 1, 2])\n","Output: tensor([[[ 0.5776, -0.1484]],\n","\n","        [[ 0.8148, -0.9833]],\n","\n","        [[ 0.8532, -0.9952]]])\n","Hidden size:  torch.Size([1, 1, 2])\n","Hidden:  tensor([[[ 0.8532, -0.9952]]])\n"]}]},{"cell_type":"code","source":["import torch\n","batch_size = 1\n","seq_len = 3\n","input_size = 4\n","hidden_size = 2\n","num_layers = 1\n","cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size,\n","num_layers=num_layers,batch_first=True)\n","# (seqLen, batchSize, inputSize)\n","inputs = torch.randn( batch_size,seq_len, input_size)\n","hidden = torch.zeros(num_layers, batch_size, hidden_size)\n","with torch. no_grad():\n","  out, hidden = cell(inputs, hidden)\n","  print('Output size:', out.shape)\n","  print('Output:', out)\n","  print('Hidden size: ', hidden.shape)\n","  print('Hidden: ', hidden)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFAOe6_xI6-s","executionInfo":{"status":"ok","timestamp":1676108207974,"user_tz":-60,"elapsed":5,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"7c120da3-10a9-4e6c-bac7-144e5bbb4bc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output size: torch.Size([1, 3, 2])\n","Output: tensor([[[ 0.1294,  0.4429],\n","         [-0.0772, -0.4156],\n","         [-0.6343, -0.9377]]])\n","Hidden size:  torch.Size([1, 1, 2])\n","Hidden:  tensor([[[-0.6343, -0.9377]]])\n"]}]},{"cell_type":"markdown","source":["# Exercise"],"metadata":{"id":"_2ttDBCDO_JP"}},{"cell_type":"markdown","source":["Creating two network using RNN cells and RNN Module respectively to mapping **'hello**' to '**ohlol**'"],"metadata":{"id":"7my4EoO4D-lN"}},{"cell_type":"markdown","source":["## 1. Using RNN Cells\n","\n","RNN Cells is more complex to implement, recommendated is to use RNN Module instead "],"metadata":{"id":"C1V9rSrREeGy"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# Parameters\n","input_size = 4\n","hidden_size = 4\n","batch_size = 1 # currently 1 \n","#num_layers = 2"],"metadata":{"id":"yT7dsMUMEjEP","executionInfo":{"status":"ok","timestamp":1676294894378,"user_tz":-60,"elapsed":424,"user":{"displayName":"liam pan","userId":"04789786804288950917"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["####1.1word embedding"],"metadata":{"id":"kbu4Z-ZlbKaP"}},{"cell_type":"code","source":["\n","idx2char = ['e', 'h', 'l', 'o']\n","x_data = [1, 0, 2, 2, 3] # hello   input\n","y_data = [3, 1, 2, 3, 2] # ohlol   target\n","\n","# one_hot_lookup = [[1, 0, 0, 0],      \n","# [0, 1, 0, 0],\n","# [0, 0, 1, 0],\n","# [0, 0, 0, 1]]\n","one_hot_lookup = np.eye(4).tolist() #   Smarter way : create numpy array and transfer it to list\n","x_one_hot = [one_hot_lookup[x] for x in x_data]\n","inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)  # seq_len,batch_sze,input_size\n","labels = torch.LongTensor(y_data).view(-1, 1)  # seq_len, 1\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSNb89Q5PDBb","executionInfo":{"status":"ok","timestamp":1676294908747,"user_tz":-60,"elapsed":259,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"5bba7a9d-f38e-4409-f4a2-7fe6de30632c"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3],\n","        [1],\n","        [2],\n","        [3],\n","        [2]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["**None related** : way to mapping index to item"],"metadata":{"id":"bZxPtlWZFvZV"}},{"cell_type":"code","source":["\n","List1 =[\"a\", \"b\", \"c\",\"d\"]\n","list2=[3,1,0,2]\n","\n","list3=[List1[x]for x in list2]\n","list3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4xZL56y5TjQf","executionInfo":{"status":"ok","timestamp":1676047262426,"user_tz":-60,"elapsed":3,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"6af2b482-6ed1-4ef6-ba89-43449a5e7119"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['d', 'b', 'a', 'c']"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["#### 1.2 Creating Model using RNNcells"],"metadata":{"id":"FdAS2DncZI4F"}},{"cell_type":"code","source":["\n","\n","\n","\n","class Model(torch.nn.Module):\n","  def __init__(self, input_size, hidden_size, batch_size,):\n","    super(Model, self).__init__()\n","    self.batch_size = batch_size\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnncell = torch.nn.RNNCell(input_size=self.input_size,\n","                   hidden_size=self.hidden_size,\n","                   )\n","    \n","  def forward(self, input,hidden):   # how to built in this structure with multi layers ? \n","    hidden = self.rnncell(input, hidden) \n","    return hidden\n","  \n","  def init_hidden(self):\n","\n","    return torch.zeros(self.batch_size,self.hidden_size)\n","\n","\n"],"metadata":{"id":"qVTttLjBbPWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize Model &Loss & Optimizer\n","RNNNet_with_rnncells = Model(input_size, hidden_size, batch_size)\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(RNNNet_with_rnncells.parameters(), lr=0.01)\n"],"metadata":{"id":"IHAajRjdblZr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1.3 Trainning Cycle\n","\n","* why loss can be computed with label being a index and hidden as a vector of n=4 ??\n","\n","\n","\n","\n","\n","* Iterate over seq_lenth, beacuse : inputs = seq_len, batch_size, input_size, labels = (seq_len.1)"],"metadata":{"id":"4i6fITBqKs5H"}},{"cell_type":"code","source":["for epoch in range(30):\n","  loss = 0\n","  optimizer.zero_grad()\n","  hidden = RNNNet_with_rnncells.init_hidden()\n","  print('Predicted string: ', end='')\n","  for input, label in zip(inputs, labels): #input: batch_size * input_size\n","\n","    # For Debugging Purpose\n","\n","    #print(label.shape)\n","    hidden = RNNNet_with_rnncells(input, hidden)\n","    #print(hidden.shape)\n","    loss += criterion(hidden, label)   # 1.no.item() at the end beacause we need to build computatioon map 2. why loss can be computed here ?\n","    _, idx = hidden.max(dim=1)\n","    print(idx2char[idx.item()], end='')\n","  loss.backward()\n","  optimizer.step()\n","  print(', Epoch [%d/15] loss=%.4f' % (epoch+1, loss.item()))"],"metadata":{"id":"0trhzHPtK57W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Using RNN Module"],"metadata":{"id":"dAqtK5xiT_wb"}},{"cell_type":"markdown","source":["#### 2.1 Word Embedding\n","\n","\n"],"metadata":{"id":"PsuJRLzJUE7k"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# Parameters\n","input_size = 4\n","hidden_size = 4\n","batch_size = 1 # currently 1 \n","num_layers = 2\n","\n","torch.zeros(num_layers,batch_size,hidden_size).shape\n","\n","\n","# word = ['h','e','l','l','0']\n","# target = ['o','h','l','o','l']\n","\n","idx2char = ['e','h','l','o']\n","x_data = [1, 0, 2, 2, 3]\n","y_data = [3, 1, 2, 3, 2]\n","\n","# transform to one hot vector\n","\n","one_hot_coder = np.eye(4).tolist()\n","\n","inputs = [one_hot_coder[x]for x in x_data]\n","inputs = torch.Tensor(inputs).view(-1,batch_size,input_size)\n","targets = torch.LongTensor(y_data)  #  keep it as a tensor , you dont have to reshape it, because now we compute loss for one sequence at one time! \n","\n"],"metadata":{"id":"4YVHD9R1U3q3","executionInfo":{"status":"ok","timestamp":1676294930782,"user_tz":-60,"elapsed":262,"user":{"displayName":"liam pan","userId":"04789786804288950917"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["#### 2.2 Creating Model using RNN Module\n","\n","documents: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"],"metadata":{"id":"qHRQa261UKm0"}},{"cell_type":"code","source":["class Model_RNN_MODULE(torch.nn.Module):\n","  def __init__(self, input_size, hidden_size,batch_size,  num_layers=1):\n","    super(Model_RNN_MODULE, self).__init__()   #  IF you changed class name , you should also change content of this line\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.batch_size = batch_size\n","\n","    self.rnnmodule = torch.nn.RNN(input_size=self.input_size,\n","                   hidden_size=self.hidden_size,\n","                   num_layers=self.num_layers\n","                   )\n","    \n","  def forward(self, input): \n","    hidden = torch.zeros(self.num_layers,self.batch_size,self.hidden_size)   # follow this sequecen num_layers , batch_size, hidden_size\n","    out, _ = self.rnnmodule(input, hidden)   # output:(seq_L,Hidden_size) ; _.aka: h_n (num_layers * Hidden_size)\n","    return out.view(-1,self.hidden_size)   # the purpose of using view is to fit setting of to bidirectional network, otherwise out is of good shape\n"],"metadata":{"id":"CSlIA4hBYwUh","executionInfo":{"status":"ok","timestamp":1676294935412,"user_tz":-60,"elapsed":244,"user":{"displayName":"liam pan","userId":"04789786804288950917"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Initialize Model &Loss & Optimizer\n","RNNNet_with_rnnmodule = Model_RNN_MODULE(input_size, hidden_size,batch_size=1,num_layers=2)\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(RNNNet_with_rnnmodule.parameters(), lr=0.05)\n"],"metadata":{"id":"1jQnzoRDaPgc","executionInfo":{"status":"ok","timestamp":1676294940294,"user_tz":-60,"elapsed":247,"user":{"displayName":"liam pan","userId":"04789786804288950917"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#### 2.3 Trainning Cycle\n","\n"],"metadata":{"id":"Iz5d38x0UUDq"}},{"cell_type":"code","source":["for epoch in range(5):\n","  optimizer.zero_grad()\n","  predicts = RNNNet_with_rnnmodule(inputs)  # feed the whole inputs including seq_len at one time\n","  \n","  # For only Debugging Purpose\n","  print(targets.shape)\n","  print(predicts.shape)\n","\n","  loss = criterion(predicts,targets)   #  compare predict with all labels along seq_lenth\n","  loss.backward()\n","  optimizer.step()\n","  #print(predicts.shape)\n","  _, idx = predicts.max(dim=1)\n","  idx = idx.data.numpy()\n","  #print(idx.shape)\n","  print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')\n","  print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))\n","\n"],"metadata":{"id":"T8beQDjYZuQj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676295043290,"user_tz":-60,"elapsed":245,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"ab578c47-714d-4c13-f5a2-62d549335c50"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5])\n","torch.Size([5, 4])\n","Predicted:  ohlol, Epoch [1/15] loss = 0.344\n","torch.Size([5])\n","torch.Size([5, 4])\n","Predicted:  ohlol, Epoch [2/15] loss = 0.343\n","torch.Size([5])\n","torch.Size([5, 4])\n","Predicted:  ohlol, Epoch [3/15] loss = 0.343\n","torch.Size([5])\n","torch.Size([5, 4])\n","Predicted:  ohlol, Epoch [4/15] loss = 0.343\n","torch.Size([5])\n","torch.Size([5, 4])\n","Predicted:  ohlol, Epoch [5/15] loss = 0.343\n"]}]},{"cell_type":"markdown","source":["## 3. Using embedding and linear layer\n","\n","1. Embedding for dense representation\n","2. Linear Layer for hiddensize inequals to class_num\n","\n","3. Afterr add Softmax layer at the end of the newt it still works well, No conflict between CrossEntropyLoss and Softamax (remember to reshape before sofmax)\n","\n","\n","\n","\n","\n","#### 3.1 Data Preparation"],"metadata":{"id":"K69Kjrzl3XtT"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# Parameters\n","num_class = 4\n","input_size = 4\n","hidden_size = 8\n","batch_size = 1 # currently 1 \n","num_layers = 2\n","embedding_size = 10\n","seq_len = 5\n","\n","torch.zeros(num_layers,batch_size,hidden_size).shape\n","\n","\n","# word = ['h','e','l','l','0']\n","# target = ['o','h','l','o','l']\n","\n","idx2char = ['e','h','l','o']\n","x_data = [1, 0, 2, 2, 3]\n","y_data = [3, 1, 2, 3, 2]\n","\n","# transform to one hot vector\n","\n","\n","inputs = torch.LongTensor(x_data).view(-1,batch_size)\n","targets = torch.LongTensor(y_data)  #  keep it as a tensor , you dont have to reshape it, because now we compute loss for one sequence at one time! \n","inputs\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cQboqtst3ifB","executionInfo":{"status":"ok","timestamp":1676109807609,"user_tz":-60,"elapsed":544,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"0e6303d6-48b6-482f-8d72-87db360691e7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1],\n","        [0],\n","        [2],\n","        [2],\n","        [3]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["#### 3.0 Test of embedding block's setting\n","\n","to make sure that, the embedding's setting fits to the further model settings\n","https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"],"metadata":{"id":"nxCE_XkO7k-s"}},{"cell_type":"code","source":["embedding = torch.nn.Embedding(num_embeddings=4, embedding_dim=10) \n","# First Para:  num_embeddings is the number of indexes that need to be embedded respectively.here it must be greater 10 since input \n","# has 10 indexe i.e. from 0(default exist) to 9 \n","\n","# Second Para: embedding_dim indicates the size of each embedded vector\n","\n","\n","# a batch of 2 samples of 4 indices each\n","#_________________________________________________\n","\n","#input = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) #  sequen_lenth *  Btach_size *(1 elemnt index)\n","print('*'*20,' Each embedded index is a vector of size embedding_dim','*'*20)\n","embedded = embedding(inputs) # #  sequen_lenth *  Btach_size * embedding_dim\n","print('*'*20,\"Input shape:sequen_lenth *  Btach_size *(1 elemnt index)\",'*'*20)\n","print('Input shape:',inputs.shape)\n","print('Input',inputs)\n","print('*'*20,\"embedded shape:sequen_lenth *  Btach_size * embedding_dim\",'*'*20)\n","print('*'*20,\"embedding_dim = input as one hot vectoc 's size for RNN\",'*'*20)\n","print('embedded: shape:  ',embedded.shape)\n","print('embedded: ',embedded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vbbQl2Cb7IhJ","executionInfo":{"status":"ok","timestamp":1676113363164,"user_tz":-60,"elapsed":518,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"6ae450a3-d19e-4c92-8f12-c73ed2ce91ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["********************  Each embedded index is a vector of size embedding_dim ********************\n","******************** Input shape:sequen_lenth *  Btach_size *(1 elemnt index) ********************\n","Input shape: torch.Size([5, 1])\n","Input tensor([[1],\n","        [0],\n","        [2],\n","        [2],\n","        [3]])\n","******************** embedded shape:sequen_lenth *  Btach_size * embedding_dim ********************\n","******************** embedding_dim = input as one hot vectoc 's size for RNN ********************\n","embedded: shape:   torch.Size([5, 1, 10])\n","embedded:  tensor([[[ 1.2499,  0.8453, -0.5364,  1.0678, -1.8656, -0.0077,  0.1174,\n","           2.1582, -0.6782, -0.1061]],\n","\n","        [[-1.2475, -0.1077, -0.1182,  1.0670,  0.4534, -0.7649, -0.7650,\n","           0.3220, -1.2432,  1.3134]],\n","\n","        [[ 0.6486,  0.2889,  0.5669,  0.5795,  1.4389, -0.5236,  0.7366,\n","          -0.5423, -1.4522,  0.0178]],\n","\n","        [[ 0.6486,  0.2889,  0.5669,  0.5795,  1.4389, -0.5236,  0.7366,\n","          -0.5423, -1.4522,  0.0178]],\n","\n","        [[-0.2483,  0.1056, -1.2202, -0.6614,  0.7520,  1.7059,  2.1364,\n","          -1.3874, -0.0411, -0.0610]]], grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"markdown","source":["#### 3.2 Creating Model using RNN Module + Embedding + Linear Layer\n","\n","\n","**Added a softmax layer before the output, this will still work,** and will not cause trouble in CrossEntropyLoss computation ( noramlly CNL computes softmax automatically, i.e. the data of your output of the model which you feed to CNL Loss wasn't normalized by softmax by the definition of CEL) , but if you choose to normalize , it still works well"],"metadata":{"id":"4BoZvvn47vis"}},{"cell_type":"code","source":["class Model_RNN_EMBED(torch.nn.Module):\n","  def __init__(self, input_size, hidden_size,batch_size,num_embeddings, embedding_dim, num_class, num_layers=1):\n","    super(Model_RNN_EMBED, self).__init__()   #  IF you changed class name , you should also change content of this line\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.batch_size = batch_size\n","    self.embedding_dim = embedding_dim\n","    self.num_embeddings = num_embeddings\n","    self.num_class = num_class\n","\n","    self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim) \n","    self.rnnmodule_emb = torch.nn.RNN(input_size = embedding_dim,\n","                   hidden_size = hidden_size,\n","                   num_layers = num_layers\n","                   )\n","    self.fc = torch.nn.Linear(hidden_size,num_class)\n","    \n","  def forward(self, input): \n","    hidden = torch.zeros(num_layers,batch_size,hidden_size)   # follow this sequecen num_layers , batch_size, hidden_size\n","    input = self.embedding(input)\n","    out, _ = self.rnnmodule_emb(input, hidden)   # output:(seq_L,Hidden_size) ; _.aka: h_n (num_layers * Hidden_size)\n","    out = self.fc(out)\n","\n","    #return out.view(-1,self.num_class)    this is the output if we skip normalization \n","\n","\n","    #*********************************  SOFTMAX  ****************************************\n","    # If we choose  normalization ,then use softmax before out , it still works , But caution! , you need to RESHAPE output first before softmax\n","    out  = out.view(-1,self.num_class) \n","    out = torch.nn.functional.softmax(out,dim=1)\n","    return out \n"],"metadata":{"id":"uMq6BFak79xb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize Model &Loss & Optimizer\n","RNNNet_with_embed = Model_RNN_EMBED(input_size, hidden_size,batch_size=1,num_layers=2,num_embeddings=4,embedding_dim=embedding_size,num_class=4)\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(RNNNet_with_embed .parameters(), lr=0.05)"],"metadata":{"id":"O_d4uUwK8HS8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3.3 Trainning Cycle"],"metadata":{"id":"H54FAQOj8BGs"}},{"cell_type":"code","source":["for epoch in range(40):\n","  optimizer.zero_grad()\n","  predicts = RNNNet_with_embed(inputs)  # feed the whole inputs including seq_len at one time\n","  loss = criterion(predicts,targets)   #  compare predict with all labels along seq_lenth\n","  loss.backward()\n","  optimizer.step()\n","  #print(predicts.shape)\n","  _, idx = predicts.max(dim=1)\n","  idx = idx.data.numpy()\n","  #print(idx.shape)\n","  print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')\n","  print(', Epoch [%d/50] loss = %.3f' % (epoch + 1, loss.item()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPyGZuVi8BgS","executionInfo":{"status":"ok","timestamp":1676112580339,"user_tz":-60,"elapsed":345,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"dc531980-2988-40e9-a735-5385c6eba7e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted:  lhlol, Epoch [1/50] loss = 1.059\n","Predicted:  lhlol, Epoch [2/50] loss = 0.983\n","Predicted:  ohlol, Epoch [3/50] loss = 0.916\n","Predicted:  ohlol, Epoch [4/50] loss = 0.857\n","Predicted:  ohlol, Epoch [5/50] loss = 0.814\n","Predicted:  ohlol, Epoch [6/50] loss = 0.785\n","Predicted:  ohlol, Epoch [7/50] loss = 0.769\n","Predicted:  ohlol, Epoch [8/50] loss = 0.760\n","Predicted:  ohlol, Epoch [9/50] loss = 0.754\n","Predicted:  ohlol, Epoch [10/50] loss = 0.751\n","Predicted:  ohlol, Epoch [11/50] loss = 0.749\n","Predicted:  ohlol, Epoch [12/50] loss = 0.748\n","Predicted:  ohlol, Epoch [13/50] loss = 0.747\n","Predicted:  ohlol, Epoch [14/50] loss = 0.746\n","Predicted:  ohlol, Epoch [15/50] loss = 0.746\n","Predicted:  ohlol, Epoch [16/50] loss = 0.746\n","Predicted:  ohlol, Epoch [17/50] loss = 0.745\n","Predicted:  ohlol, Epoch [18/50] loss = 0.745\n","Predicted:  ohlol, Epoch [19/50] loss = 0.745\n","Predicted:  ohlol, Epoch [20/50] loss = 0.745\n","Predicted:  ohlol, Epoch [21/50] loss = 0.745\n","Predicted:  ohlol, Epoch [22/50] loss = 0.745\n","Predicted:  ohlol, Epoch [23/50] loss = 0.744\n","Predicted:  ohlol, Epoch [24/50] loss = 0.744\n","Predicted:  ohlol, Epoch [25/50] loss = 0.744\n","Predicted:  ohlol, Epoch [26/50] loss = 0.744\n","Predicted:  ohlol, Epoch [27/50] loss = 0.744\n","Predicted:  ohlol, Epoch [28/50] loss = 0.744\n","Predicted:  ohlol, Epoch [29/50] loss = 0.744\n","Predicted:  ohlol, Epoch [30/50] loss = 0.744\n","Predicted:  ohlol, Epoch [31/50] loss = 0.744\n","Predicted:  ohlol, Epoch [32/50] loss = 0.744\n","Predicted:  ohlol, Epoch [33/50] loss = 0.744\n","Predicted:  ohlol, Epoch [34/50] loss = 0.744\n","Predicted:  ohlol, Epoch [35/50] loss = 0.744\n","Predicted:  ohlol, Epoch [36/50] loss = 0.744\n","Predicted:  ohlol, Epoch [37/50] loss = 0.744\n","Predicted:  ohlol, Epoch [38/50] loss = 0.744\n","Predicted:  ohlol, Epoch [39/50] loss = 0.744\n","Predicted:  ohlol, Epoch [40/50] loss = 0.744\n"]}]},{"cell_type":"markdown","source":["##for Multi-Classification  how to get index of output form probability tensor ：output \n","\n","\n","```\n","# what,idx = outputs.max(dim=1)\n","```\n","\n","\n","\n","what 包含所有最大值， idx 为那维度各个位置的最大值对应的index\n"],"metadata":{"id":"btTEBIELe5RR"}},{"cell_type":"code","source":["\n","outputs = torch.randn(5,5)\n","print(outputs)\n","what,idx = outputs.max(dim=1)  # 返回 dim=1 方向上的 最大值组成的tensor放在what中，把他们的index放在idx中\n","print(what)\n","print(idx)\n","\n","print(\"*\"*20,\"  create a tensor filled with random number and apply max to it ，so the method is verified\")\n","\n","output_size = outputs.shape # get outputs's size\n","\n","test2 =torch.randn(output_size) # create a random tensor of outputs's size\n","\n","print(test2)\n","max_test2,idx = test2.max(dim=1)\n","\n","print(max_test2) # get it's maxima\n","print(idx)# see its index of maxima\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qr7h4u94eAsc","executionInfo":{"status":"ok","timestamp":1676053880427,"user_tz":-60,"elapsed":3,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"079d5de6-ee4d-4129-f306-22722fd21a0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-1.0618, -0.1526, -0.2571,  0.3936,  0.5633],\n","        [ 0.6730,  0.1595, -0.0259, -1.2831, -0.5916],\n","        [-1.8953, -0.6111,  0.9884,  0.5086, -0.1596],\n","        [ 0.1374, -0.0319,  0.1647, -0.1492, -0.2406],\n","        [-0.5378, -1.8251, -0.9900,  1.4081,  0.4630]])\n","tensor([0.5633, 0.6730, 0.9884, 0.1647, 1.4081])\n","tensor([4, 0, 2, 2, 3])\n","********************   create a tensor filled with random number and apply max to it ，so the method is verified\n","tensor([[ 0.2991, -1.4580, -0.4263,  1.4588,  0.0297],\n","        [-0.5792,  0.9098,  0.4497, -0.7781,  1.3429],\n","        [-0.1592,  0.9286,  0.3492, -0.8756, -1.2354],\n","        [ 1.8106, -0.1697,  1.1576,  0.4994, -0.6966],\n","        [-1.5289,  0.4375, -0.8623, -1.1259,  0.4140]])\n","tensor([1.4588, 1.3429, 0.9286, 1.8106, 0.4375])\n","tensor([3, 4, 1, 0, 1])\n"]}]},{"cell_type":"markdown","source":["## Realization of Mapping indexes to values\n","\n","idx is stored in  a list ，\n","```\n","idx = [3,2,0,3]\n","idx2char = ['e','h','l','o']\n","# print( ''.join([idx2char[x] for x in idx]), end='')\n","```\n","\n","\n"],"metadata":{"id":"MMfHCFsKmR4G"}},{"cell_type":"code","source":["idx = [3,2,0,3]\n","idx2char = ['e','h','l','o']\n","print( ''.join([idx2char[x] for x in idx]), end='')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RTM-IwTumrHO","executionInfo":{"status":"ok","timestamp":1676054187540,"user_tz":-60,"elapsed":3,"user":{"displayName":"liam pan","userId":"04789786804288950917"}},"outputId":"d02c658c-b34e-4385-ec9a-fc42dfae019d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["oleo"]}]}]}